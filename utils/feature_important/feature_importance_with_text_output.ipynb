{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/miniconda3/envs/finetune-dephos/lib/python3.9/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/ubuntu/data/miniconda3/envs/finetune-dephos/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2556\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2556\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2554\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518780 -> initscore=0.075155\n",
      "[LightGBM] [Info] Start training from score 0.075155\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2556\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2556\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2556\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7609\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7619\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000848 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7618\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518780 -> initscore=0.075155\n",
      "[LightGBM] [Info] Start training from score 0.075155\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7594\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000885 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7614\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7662\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12682\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12703\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12693\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518780 -> initscore=0.075155\n",
      "[LightGBM] [Info] Start training from score 0.075155\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12658\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12683\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12774\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22901\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22923\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22911\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518780 -> initscore=0.075155\n",
      "[LightGBM] [Info] Start training from score 0.075155\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22875\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22902\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22994\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 43333\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 221\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 43357\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 221\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 43337\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 221\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518780 -> initscore=0.075155\n",
      "[LightGBM] [Info] Start training from score 0.075155\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 43301\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 221\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 43325\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 221\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 43421\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 221\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 84199\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 429\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3673, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 84217\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 429\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518639 -> initscore=0.074590\n",
      "[LightGBM] [Info] Start training from score 0.074590\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3408\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 84184\n",
      "[LightGBM] [Info] Number of data points in the train set: 7082, number of used features: 429\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518780 -> initscore=0.075155\n",
      "[LightGBM] [Info] Start training from score 0.075155\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 84148\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 429\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 3674, number of negative: 3409\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 84166\n",
      "[LightGBM] [Info] Number of data points in the train set: 7083, number of used features: 429\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518707 -> initscore=0.074862\n",
      "[LightGBM] [Info] Start training from score 0.074862\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 84278\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 429\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "\n",
      "=== FEATURE IMPORTANCE ANALYSIS ===\n",
      "\n",
      "=== RANDOM_FOREST ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Feature         Win±0               Win±1               Win±2               Win±4               Win±8               Win±16              \n",
      "----------------------------------------------------------------------------------------------------\n",
      "chi1            0.0962±0.0000 0.0323±0.0019 0.0199±0.0005 0.0110±0.0005 0.0060±0.0003 0.0030±0.0003 \n",
      "chi2            0.0940±0.0000 0.0315±0.0018 0.0195±0.0009 0.0110±0.0009 0.0057±0.0005 0.0030±0.0004 \n",
      "chi3            0.0928±0.0000 0.0251±0.0060 0.0142±0.0036 0.0079±0.0020 0.0042±0.0014 0.0021±0.0008 \n",
      "chi4            0.0951±0.0000 0.0198±0.0107 0.0113±0.0064 0.0056±0.0037 0.0029±0.0020 0.0013±0.0010 \n",
      "omega           0.0953±0.0000 0.0340±0.0009 0.0203±0.0009 0.0112±0.0006 0.0061±0.0003 0.0032±0.0002 \n",
      "phi             0.0902±0.0000 0.0328±0.0005 0.0200±0.0005 0.0110±0.0004 0.0058±0.0004 0.0032±0.0003 \n",
      "plDDT           0.1295±0.0000 0.0455±0.0048 0.0273±0.0014 0.0156±0.0018 0.0082±0.0014 0.0043±0.0013 \n",
      "psi             0.0894±0.0000 0.0319±0.0005 0.0192±0.0002 0.0107±0.0004 0.0057±0.0003 0.0030±0.0003 \n",
      "sasa            0.1049±0.0000 0.0420±0.0015 0.0249±0.0034 0.0140±0.0026 0.0071±0.0019 0.0036±0.0010 \n",
      "ss              0.0064±0.0012 0.0021±0.0005 0.0013±0.0003 0.0008±0.0002 0.0004±0.0001 0.0002±0.0001 \n",
      "tau             0.0932±0.0000 0.0320±0.0007 0.0196±0.0005 0.0109±0.0003 0.0058±0.0004 0.0030±0.0003 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== XGBOOST ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Feature         Win±0               Win±1               Win±2               Win±4               Win±8               Win±16              \n",
      "----------------------------------------------------------------------------------------------------\n",
      "chi1            0.0730±0.0000 0.0218±0.0009 0.0138±0.0008 0.0082±0.0007 0.0044±0.0007 0.0024±0.0005 \n",
      "chi2            0.0760±0.0000 0.0216±0.0003 0.0148±0.0017 0.0087±0.0014 0.0045±0.0007 0.0025±0.0006 \n",
      "chi3            0.0722±0.0000 0.0228±0.0024 0.0166±0.0038 0.0106±0.0024 0.0053±0.0012 0.0028±0.0012 \n",
      "chi4            0.0748±0.0000 0.0543±0.0347 0.0305±0.0199 0.0182±0.0098 0.0083±0.0046 0.0035±0.0024 \n",
      "omega           0.0799±0.0000 0.0219±0.0007 0.0139±0.0008 0.0076±0.0007 0.0041±0.0007 0.0023±0.0004 \n",
      "phi             0.0720±0.0000 0.0222±0.0006 0.0140±0.0009 0.0074±0.0006 0.0041±0.0004 0.0024±0.0005 \n",
      "plDDT           0.1209±0.0000 0.0336±0.0124 0.0234±0.0127 0.0138±0.0083 0.0092±0.0136 0.0047±0.0073 \n",
      "psi             0.0720±0.0000 0.0217±0.0006 0.0144±0.0010 0.0076±0.0009 0.0046±0.0008 0.0023±0.0006 \n",
      "sasa            0.0833±0.0000 0.0274±0.0028 0.0171±0.0021 0.0098±0.0017 0.0054±0.0013 0.0030±0.0010 \n",
      "ss              0.0688±0.0101 0.0217±0.0089 0.0091±0.0074 0.0037±0.0045 0.0015±0.0024 0.0007±0.0015 \n",
      "tau             0.0697±0.0000 0.0209±0.0005 0.0143±0.0008 0.0080±0.0010 0.0044±0.0007 0.0024±0.0005 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== LIGHTGBM ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Feature         Win±0               Win±1               Win±2               Win±4               Win±8               Win±16              \n",
      "----------------------------------------------------------------------------------------------------\n",
      "chi1            247.0000±0.0000 108.0000±7.7889 61.4000±5.9867 36.3333±4.3716 17.7647±4.2776 9.3939±3.4372 \n",
      "chi2            330.0000±0.0000 121.3333±15.0849 82.0000±10.2567 46.3333±8.9691 23.1765±7.0979 12.0000±5.3598 \n",
      "chi3            325.0000±0.0000 64.3333±20.5318 39.4000±18.7574 26.4444±8.5129 13.3529±7.7758 6.7273±5.5338 \n",
      "chi4            292.0000±0.0000 87.3333±33.3700 47.2000±22.5513 28.3333±17.7326 17.6471±12.1507 9.6667±10.7891 \n",
      "omega           323.0000±0.0000 107.3333±1.2472 64.6000±5.5353 30.8889±8.3725 16.8824±4.4043 8.5758±3.7902 \n",
      "phi             301.0000±0.0000 95.0000±14.3527 62.2000±6.7941 37.8889±7.1250 17.8824±6.4433 9.7273±3.5013 \n",
      "plDDT           307.0000±0.0000 83.0000±27.4348 45.2000±10.0080 24.5556±6.0573 13.4706±4.9955 6.6970±3.3165 \n",
      "psi             270.0000±0.0000 88.0000±3.2660 58.0000±7.4027 28.8889±3.2470 14.6471±4.1008 7.0000±3.5929 \n",
      "sasa            342.0000±0.0000 145.0000±13.8804 82.8000±3.9699 44.8889±10.3112 27.2353±6.5756 13.1818±6.5248 \n",
      "ss              6.6667±5.9067 2.3333±2.3094 1.6000±1.2543 0.7778±0.8315 0.3333±0.6468 0.1919±0.5061 \n",
      "tau             243.0000±0.0000 93.6667±5.3125 52.4000±7.0880 26.4444±5.6196 13.4118±4.1025 7.3636±3.6166 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== GRADIENT_BOOSTING ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Feature         Win±0               Win±1               Win±2               Win±4               Win±8               Win±16              \n",
      "----------------------------------------------------------------------------------------------------\n",
      "chi1            0.0538±0.0000 0.0161±0.0017 0.0094±0.0034 0.0048±0.0030 0.0022±0.0017 0.0017±0.0014 \n",
      "chi2            0.0622±0.0000 0.0247±0.0128 0.0193±0.0095 0.0104±0.0050 0.0047±0.0039 0.0021±0.0022 \n",
      "chi3            0.0539±0.0000 0.0133±0.0017 0.0098±0.0084 0.0069±0.0079 0.0037±0.0057 0.0019±0.0039 \n",
      "chi4            0.0683±0.0000 0.0691±0.0638 0.0455±0.0528 0.0263±0.0409 0.0155±0.0270 0.0075±0.0199 \n",
      "omega           0.0824±0.0000 0.0144±0.0063 0.0068±0.0025 0.0032±0.0032 0.0023±0.0017 0.0010±0.0012 \n",
      "phi             0.0362±0.0000 0.0160±0.0036 0.0091±0.0034 0.0052±0.0033 0.0024±0.0019 0.0011±0.0012 \n",
      "plDDT           0.4276±0.0000 0.0967±0.0903 0.0498±0.0521 0.0287±0.0382 0.0146±0.0270 0.0075±0.0191 \n",
      "psi             0.0401±0.0000 0.0121±0.0026 0.0076±0.0018 0.0036±0.0018 0.0018±0.0015 0.0009±0.0015 \n",
      "sasa            0.1482±0.0000 0.0591±0.0104 0.0363±0.0197 0.0189±0.0181 0.0094±0.0123 0.0056±0.0093 \n",
      "ss              0.0007±0.0005 0.0002±0.0004 0.0003±0.0008 0.0001±0.0004 0.0000±0.0001 0.0000±0.0002 \n",
      "tau             0.0253±0.0000 0.0113±0.0031 0.0053±0.0029 0.0029±0.0025 0.0021±0.0013 0.0009±0.0010 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== LOGISTIC ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Feature         Win±0               Win±1               Win±2               Win±4               Win±8               Win±16              \n",
      "----------------------------------------------------------------------------------------------------\n",
      "chi1            0.0350±0.0000 0.0507±0.0201 0.0545±0.0350 0.0354±0.0190 0.0343±0.0277 0.0313±0.0201 \n",
      "chi2            0.0161±0.0000 0.0543±0.0356 0.0496±0.0269 0.0615±0.0553 0.0650±0.0466 0.0518±0.0353 \n",
      "chi3            0.0047±0.0000 0.0774±0.0584 0.0548±0.0459 0.0663±0.0564 0.0540±0.0405 0.0625±0.0620 \n",
      "chi4            0.0079±0.0000 0.1189±0.0775 0.0828±0.0480 0.1516±0.1577 0.1293±0.0999 0.1211±0.1047 \n",
      "omega           0.0273±0.0000 0.0351±0.0221 0.0253±0.0124 0.0363±0.0356 0.0502±0.0499 0.0365±0.0292 \n",
      "phi             0.0058±0.0000 0.1210±0.1321 0.0623±0.0671 0.0541±0.0453 0.0459±0.0387 0.0449±0.0390 \n",
      "plDDT           0.5774±0.0000 0.4268±0.3215 0.3088±0.2410 0.2865±0.1767 0.2303±0.1656 0.2115±0.1439 \n",
      "psi             0.0116±0.0000 0.0663±0.0507 0.0917±0.0580 0.0927±0.0621 0.0816±0.0478 0.0609±0.0428 \n",
      "sasa            0.2399±0.0000 0.4652±0.1741 0.2554±0.1281 0.1511±0.1164 0.1573±0.0758 0.0842±0.0696 \n",
      "ss              0.0155±0.0097 0.1395±0.1072 0.0524±0.0373 0.0721±0.0577 0.0973±0.0810 0.1127±0.1028 \n",
      "tau             0.0491±0.0000 0.1472±0.1570 0.0846±0.0784 0.0672±0.0583 0.0917±0.0693 0.0712±0.0553 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== SHAP ===\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Feature         Win±0               Win±1               Win±2               Win±4               Win±8               Win±16              \n",
      "----------------------------------------------------------------------------------------------------\n",
      "chi1            0.1645±0.0000 0.0899±0.0184 0.0670±0.0049 0.0513±0.0074 0.0341±0.0064 0.0208±0.0103 \n",
      "chi2            0.1701±0.0000 0.1201±0.0157 0.1003±0.0188 0.0765±0.0235 0.0491±0.0164 0.0321±0.0184 \n",
      "chi3            0.1760±0.0000 0.0785±0.0133 0.0571±0.0174 0.0430±0.0154 0.0287±0.0164 0.0170±0.0142 \n",
      "chi4            0.1846±0.0000 0.1459±0.0548 0.1056±0.0702 0.0688±0.0617 0.0566±0.0449 0.0339±0.0446 \n",
      "omega           0.1747±0.0000 0.1011±0.0121 0.0712±0.0108 0.0522±0.0126 0.0370±0.0093 0.0235±0.0084 \n",
      "phi             0.1594±0.0000 0.1147±0.0071 0.0836±0.0148 0.0545±0.0132 0.0383±0.0145 0.0228±0.0103 \n",
      "plDDT           0.5057±0.0000 0.2235±0.1122 0.1402±0.0792 0.0921±0.0723 0.0509±0.0516 0.0310±0.0392 \n",
      "psi             0.1522±0.0000 0.0900±0.0071 0.0736±0.0079 0.0504±0.0104 0.0331±0.0101 0.0235±0.0118 \n",
      "sasa            0.2744±0.0000 0.1706±0.0148 0.1257±0.0270 0.0889±0.0393 0.0660±0.0377 0.0402±0.0335 \n",
      "ss              0.0158±0.0104 0.0083±0.0076 0.0025±0.0028 0.0017±0.0033 0.0008±0.0014 0.0007±0.0016 \n",
      "tau             0.1648±0.0000 0.0875±0.0011 0.0646±0.0048 0.0466±0.0119 0.0300±0.0086 0.0211±0.0116 \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename=f'feature_importance_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "class FeatureImportanceAnalyzer:\n",
    "    def __init__(self, continuous_features, discrete_features):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with feature definitions.\n",
    "        \n",
    "        Args:\n",
    "            continuous_features (list): List of continuous feature names\n",
    "            discrete_features (list): List of discrete feature names\n",
    "        \"\"\"\n",
    "        self.continuous_features = continuous_features\n",
    "        self.discrete_features = discrete_features\n",
    "        self.models = {\n",
    "            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'xgboost': xgb.XGBClassifier(n_estimators=100, random_state=42),\n",
    "            'lightgbm': lgb.LGBMClassifier(n_estimators=100, random_state=42),\n",
    "            'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "            'logistic': LogisticRegression(max_iter=1000, random_state=42)\n",
    "        }\n",
    "        \n",
    "    def prepare_data(self, df, window_size):\n",
    "        \"\"\"\n",
    "        Prepare data for analysis with proper scaling and encoding.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input dataframe\n",
    "            window_size (int): Size of the window for feature extraction\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (X, y, feature_names)\n",
    "        \"\"\"\n",
    "        X_processed = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # Calculate middle position based on sequence length\n",
    "        seq_length = len(df[self.continuous_features[0]].iloc[0])\n",
    "        middle_pos = seq_length // 2\n",
    "        \n",
    "        # Validate window size\n",
    "        if window_size * 2 + 1 > seq_length:\n",
    "            raise ValueError(f\"Window size {window_size} exceeds sequence length {seq_length}\")\n",
    "        \n",
    "        # Process continuous features with position-based weighting\n",
    "        scaler = StandardScaler()\n",
    "        weights = np.exp(-np.abs(np.arange(-window_size, window_size+1)) / window_size) if window_size > 0 else [1]\n",
    "        \n",
    "        for feature in self.continuous_features:\n",
    "            positions = list(range(middle_pos-window_size, middle_pos+window_size+1))\n",
    "            for idx, pos in enumerate(positions):\n",
    "                values = np.array([x[pos] for x in df[feature].values])\n",
    "                mask = values != 0\n",
    "                if mask.any():\n",
    "                    non_zero_values = values[mask].reshape(-1, 1)\n",
    "                    values[mask] = scaler.fit_transform(non_zero_values).flatten() * weights[idx]\n",
    "                X_processed.append(values)\n",
    "                feature_names.append(f\"{feature}_{pos-middle_pos}\")\n",
    "        \n",
    "        # Process SS with improved encoding\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        positions = list(range(middle_pos-window_size, middle_pos+window_size+1))\n",
    "        for idx, pos in enumerate(positions):\n",
    "            ss_values = np.array([x[pos] for x in df['ss'].values]).reshape(-1, 1)\n",
    "            encoded_ss = encoder.fit_transform(ss_values)\n",
    "            for i, ss_type in enumerate(encoder.categories_[0]):\n",
    "                X_processed.append(encoded_ss[:, i] * weights[idx])\n",
    "                feature_names.append(f\"ss_{ss_type}_{pos-middle_pos}\")\n",
    "        \n",
    "        X = np.array(X_processed).T\n",
    "        y = df['label'].values\n",
    "        \n",
    "        return X, y, feature_names\n",
    "    \n",
    "    def get_model_importance(self, model_name, X, y, feature_names):\n",
    "        \"\"\"\n",
    "        Get feature importance from a specific model with cross-validation.\n",
    "        \"\"\"\n",
    "        model = self.models[model_name]\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "        logging.info(f\"{model_name} CV Score: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "        \n",
    "        # Fit model and get importance\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        if model_name == 'logistic':\n",
    "            importance = np.abs(model.coef_[0])\n",
    "        else:\n",
    "            importance = model.feature_importances_\n",
    "            \n",
    "        return importance\n",
    "    \n",
    "    def get_shap_importance(self, X, y, feature_names):\n",
    "        \"\"\"\n",
    "        Calculate SHAP importance values.\n",
    "        \"\"\"\n",
    "        model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0]\n",
    "        \n",
    "        return np.abs(shap_values).mean(axis=0)\n",
    "    \n",
    "    def analyze_feature_correlations(self, X, feature_names):\n",
    "        \"\"\"\n",
    "        Analyze correlations between features.\n",
    "        \"\"\"\n",
    "        corr_matrix = pd.DataFrame(X, columns=feature_names).corr()\n",
    "        return corr_matrix\n",
    "    \n",
    "    def analyze_all_models(self, df, window_size):\n",
    "        \"\"\"\n",
    "        Analyze feature importance using all models.\n",
    "        \"\"\"\n",
    "        X, y, feature_names = self.prepare_data(df, window_size)\n",
    "        \n",
    "        results = {}\n",
    "        # Get importance from each model\n",
    "        for model_name in self.models.keys():\n",
    "            try:\n",
    "                importance = self.get_model_importance(model_name, X, y, feature_names)\n",
    "                results[model_name] = self.aggregate_importance(importance, feature_names)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in {model_name} analysis: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Add SHAP importance\n",
    "        try:\n",
    "            shap_importance = self.get_shap_importance(X, y, feature_names)\n",
    "            results['shap'] = self.aggregate_importance(shap_importance, feature_names)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"SHAP analysis failed: {e}\")\n",
    "            \n",
    "        # Add correlation analysis\n",
    "        try:\n",
    "            corr_matrix = self.analyze_feature_correlations(X, feature_names)\n",
    "            results['correlations'] = corr_matrix\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Correlation analysis failed: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def aggregate_importance(self, importance, feature_names):\n",
    "        \"\"\"\n",
    "        Aggregate feature importance with improved handling.\n",
    "        \"\"\"\n",
    "        feature_importance = {}\n",
    "        for feat_type in self.continuous_features + ['ss']:\n",
    "            if feat_type == 'ss':\n",
    "                mask = [f.startswith('ss_') for f in feature_names]\n",
    "            else:\n",
    "                mask = [f.startswith(feat_type + '_') for f in feature_names]\n",
    "            importance_values = importance[mask]\n",
    "            \n",
    "            feature_importance[feat_type] = {\n",
    "                'mean': np.mean(importance_values),\n",
    "                'std': np.std(importance_values),\n",
    "                'max': np.max(importance_values),\n",
    "                'min': np.min(importance_values)\n",
    "            }\n",
    "        return feature_importance\n",
    "\n",
    "def print_model_importance_tables(all_results):\n",
    "    \"\"\"Print comprehensive feature importance tables for each model.\"\"\"\n",
    "    model_names = list(all_results[0].keys())\n",
    "    window_sizes = list(all_results.keys())\n",
    "    \n",
    "    for model in model_names:\n",
    "        if model == 'correlations':\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n=== {model.upper()} ===\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        # Print header\n",
    "        print(f\"{'Feature':<15} \", end=\"\")\n",
    "        for size in window_sizes:\n",
    "            print(f\"Win±{size:<15} \", end=\"\")\n",
    "        print(\"\\n\" + \"-\" * 100)\n",
    "        \n",
    "        # Get all features\n",
    "        features = list(all_results[0][model].keys())\n",
    "        \n",
    "        # Print each feature's importance\n",
    "        for feature in sorted(features):\n",
    "            print(f\"{feature:<15} \", end=\"\")\n",
    "            for size in window_sizes:\n",
    "                if model in all_results[size]:\n",
    "                    importance = all_results[size][model].get(feature, {}).get('mean', 0)\n",
    "                    std = all_results[size][model].get(feature, {}).get('std', 0)\n",
    "                    print(f\"{importance:,.4f}±{std:,.4f} \", end=\"\")\n",
    "                else:\n",
    "                    print(\"N/A            \", end=\"\")\n",
    "            print()\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "def analyze_features(train_df_path):\n",
    "    \"\"\"\n",
    "    Main function to analyze feature importance.\n",
    "    \"\"\"\n",
    "    # Define features\n",
    "    continuous_features = [\n",
    "        'phi', 'psi', 'omega', 'tau', \n",
    "        'chi1', 'chi2', 'chi3', 'chi4',\n",
    "        'sasa', 'plDDT'\n",
    "    ]\n",
    "    discrete_features = ['ss']\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = FeatureImportanceAnalyzer(continuous_features, discrete_features)\n",
    "    \n",
    "    # Read and process data\n",
    "    train_df_path = os.path.abspath(os.path.expanduser(train_df_path))\n",
    "    logging.info(f\"Reading data from: {train_df_path}\")\n",
    "    \n",
    "    try:\n",
    "        train_df = pd.read_csv(train_df_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading data: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Process features\n",
    "    processed_df = pd.DataFrame()\n",
    "    processed_df['label'] = train_df['label']\n",
    "    \n",
    "    def process_feature(df, feature_name):\n",
    "        if feature_name == 'ss':\n",
    "            return df[feature_name].apply(list)\n",
    "        else:\n",
    "            return df[feature_name].apply(lambda x: \n",
    "                np.array([float(v.strip()) for v in x.strip('[]').split(',')]))\n",
    "    \n",
    "    for feature in continuous_features:\n",
    "        processed_df[feature] = process_feature(train_df, feature)\n",
    "    processed_df['ss'] = train_df['ss'].apply(list)\n",
    "    \n",
    "    # Analyze with different window sizes\n",
    "    window_sizes = [0, 1, 2, 4, 8, 16]\n",
    "    all_results = {}\n",
    "    \n",
    "    for size in window_sizes:\n",
    "        logging.info(f\"Processing window size ±{size}\")\n",
    "        try:\n",
    "            results = analyzer.analyze_all_models(processed_df, size)\n",
    "            all_results[size] = results\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing window size {size}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = 'feature_importance_results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save results and create visualizations\n",
    "    save_results(all_results, results_dir, window_sizes)\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"\\n=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "    print_model_importance_tables(all_results)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def save_results(all_results, results_dir, window_sizes):\n",
    "    \"\"\"\n",
    "    Save analysis results and create visualizations.\n",
    "    \"\"\"\n",
    "    for size in window_sizes:\n",
    "        if 'correlations' in all_results[size]:\n",
    "            corr_matrix = all_results[size]['correlations']\n",
    "            plt.figure(figsize=(15, 12))\n",
    "            sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "            plt.title(f'Feature Correlations (Window Size ±{size})')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(results_dir, f'correlation_matrix_win{size}.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Save correlation matrix\n",
    "            corr_matrix.to_csv(os.path.join(results_dir, f'correlation_matrix_win{size}.csv'))\n",
    "    \n",
    "    # Save importance results for each model\n",
    "    model_names = [name for name in all_results[0].keys() if name != 'correlations']\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        try:\n",
    "            results_df = pd.DataFrame({\n",
    "                size: {k: v['mean'] for k, v in results[model_name].items()}\n",
    "                for size, results in all_results.items()\n",
    "            })\n",
    "            \n",
    "            # Save numerical results\n",
    "            results_df.to_csv(os.path.join(results_dir, f'feature_importance_{model_name}.csv'))\n",
    "            \n",
    "            # Create heatmap\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(results_df, annot=True, fmt='.3f', cmap='YlOrRd')\n",
    "            plt.title(f'Feature Importance vs Window Size ({model_name.upper()})')\n",
    "            plt.xlabel('Window Size')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(results_dir, f'feature_importance_heatmap_{model_name}.png'))\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving results for {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results = analyze_features('../../data/train/structure/processed_features_train.csv')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Analysis failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: /home/ubuntu/data/hai/thesis/processed_features_fixed_train.csv\n",
      "Processing window size ±0\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2556\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "Processing window size ±1\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7662\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "Processing window size ±2\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12774\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "Processing window size ±4\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22994\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "Processing window size ±8\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 43421\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 221\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "Processing window size ±16\n",
      "[LightGBM] [Info] Number of positive: 4592, number of negative: 4261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 84279\n",
      "[LightGBM] [Info] Number of data points in the train set: 8853, number of used features: 429\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.518694 -> initscore=0.074812\n",
      "[LightGBM] [Info] Start training from score 0.074812\n",
      "\n",
      "=== ANALYSIS BY MODEL ===\n",
      "\n",
      "=== RANDOM_FOREST ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         Win±0            Win±1            Win±2            Win±4            Win±8            Win±16           \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            0.0962    0.0321    0.0199    0.0110    0.0060    0.0030    \n",
      "chi2            0.0940    0.0316    0.0195    0.0111    0.0057    0.0030    \n",
      "chi3            0.0928    0.0253    0.0142    0.0079    0.0042    0.0021    \n",
      "chi4            0.0951    0.0199    0.0113    0.0056    0.0029    0.0013    \n",
      "omega           0.0953    0.0338    0.0203    0.0113    0.0061    0.0032    \n",
      "phi             0.0902    0.0330    0.0200    0.0110    0.0059    0.0032    \n",
      "plDDT           0.1295    0.0458    0.0273    0.0156    0.0083    0.0043    \n",
      "psi             0.0894    0.0318    0.0193    0.0107    0.0057    0.0030    \n",
      "sasa            0.1049    0.0418    0.0249    0.0140    0.0071    0.0036    \n",
      "ss              0.0064    0.0021    0.0013    0.0008    0.0004    0.0002    \n",
      "tau             0.0932    0.0318    0.0196    0.0108    0.0058    0.0030    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== XGBOOST ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         Win±0            Win±1            Win±2            Win±4            Win±8            Win±16           \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            0.0730    0.0218    0.0149    0.0082    0.0045    0.0024    \n",
      "chi2            0.0760    0.0216    0.0151    0.0087    0.0046    0.0025    \n",
      "chi3            0.0722    0.0228    0.0174    0.0106    0.0053    0.0028    \n",
      "chi4            0.0748    0.0543    0.0303    0.0182    0.0082    0.0035    \n",
      "omega           0.0799    0.0219    0.0142    0.0076    0.0042    0.0023    \n",
      "phi             0.0720    0.0222    0.0144    0.0074    0.0042    0.0024    \n",
      "plDDT           0.1209    0.0336    0.0256    0.0138    0.0089    0.0048    \n",
      "psi             0.0720    0.0217    0.0142    0.0076    0.0047    0.0023    \n",
      "sasa            0.0833    0.0274    0.0174    0.0098    0.0055    0.0029    \n",
      "ss              0.0688    0.0217    0.0071    0.0037    0.0014    0.0007    \n",
      "tau             0.0697    0.0209    0.0153    0.0080    0.0045    0.0024    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== LIGHTGBM ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         Win±0            Win±1            Win±2            Win±4            Win±8            Win±16           \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            247.0000    102.6667    62.2000    35.0000    17.1176    8.6061    \n",
      "chi2            330.0000    124.3333    71.6000    46.6667    23.3529    11.7879    \n",
      "chi3            325.0000    69.3333    36.0000    24.4444    13.5882    6.8182    \n",
      "chi4            292.0000    87.0000    51.8000    29.0000    18.9412    9.6061    \n",
      "omega           323.0000    110.0000    63.0000    32.7778    18.5294    8.2424    \n",
      "phi             301.0000    106.6667    63.0000    36.1111    18.4706    10.1212    \n",
      "plDDT           307.0000    91.0000    50.8000    24.0000    12.4118    6.2727    \n",
      "psi             270.0000    89.3333    57.6000    31.1111    13.5294    6.7273    \n",
      "sasa            342.0000    119.3333    86.8000    44.4444    26.1765    13.7273    \n",
      "ss              6.6667    3.0000    1.4000    0.7037    0.2353    0.1313    \n",
      "tau             243.0000    91.3333    53.0000    27.6667    13.6471    8.6061    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GRADIENT_BOOSTING ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         Win±0            Win±1            Win±2            Win±4            Win±8            Win±16           \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            0.0538    0.0178    0.0095    0.0048    0.0022    0.0017    \n",
      "chi2            0.0622    0.0261    0.0193    0.0104    0.0047    0.0021    \n",
      "chi3            0.0539    0.0117    0.0099    0.0069    0.0037    0.0019    \n",
      "chi4            0.0683    0.0688    0.0455    0.0263    0.0155    0.0075    \n",
      "omega           0.0824    0.0143    0.0068    0.0032    0.0023    0.0010    \n",
      "phi             0.0362    0.0154    0.0091    0.0052    0.0024    0.0011    \n",
      "plDDT           0.4276    0.0968    0.0498    0.0287    0.0146    0.0075    \n",
      "psi             0.0401    0.0114    0.0074    0.0036    0.0018    0.0009    \n",
      "sasa            0.1482    0.0591    0.0362    0.0189    0.0094    0.0056    \n",
      "ss              0.0007    0.0002    0.0003    0.0001    0.0000    0.0000    \n",
      "tau             0.0253    0.0115    0.0054    0.0029    0.0021    0.0009    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== LOGISTIC ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         Win±0            Win±1            Win±2            Win±4            Win±8            Win±16           \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            0.0350    0.0253    0.0283    0.0209    0.0205    0.0191    \n",
      "chi2            0.0161    0.0219    0.0237    0.0312    0.0353    0.0309    \n",
      "chi3            0.0047    0.0305    0.0273    0.0344    0.0321    0.0355    \n",
      "chi4            0.0079    0.0507    0.0449    0.0733    0.0728    0.0702    \n",
      "omega           0.0273    0.0187    0.0149    0.0196    0.0270    0.0227    \n",
      "phi             0.0058    0.0459    0.0335    0.0297    0.0273    0.0265    \n",
      "plDDT           0.5774    0.1916    0.1850    0.1751    0.1561    0.1437    \n",
      "psi             0.0116    0.0252    0.0424    0.0494    0.0460    0.0375    \n",
      "sasa            0.2399    0.2230    0.1610    0.1052    0.0991    0.0610    \n",
      "ss              0.0155    0.0629    0.0292    0.0378    0.0555    0.0660    \n",
      "tau             0.0491    0.0683    0.0497    0.0416    0.0526    0.0444    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== SHAP ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         Win±0            Win±1            Win±2            Win±4            Win±8            Win±16           \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            0.1645    0.0899    0.0682    0.0513    0.0337    0.0210    \n",
      "chi2            0.1701    0.1201    0.1042    0.0765    0.0509    0.0320    \n",
      "chi3            0.1760    0.0785    0.0533    0.0430    0.0285    0.0168    \n",
      "chi4            0.1846    0.1459    0.1067    0.0688    0.0581    0.0338    \n",
      "omega           0.1747    0.1011    0.0743    0.0522    0.0367    0.0231    \n",
      "phi             0.1594    0.1147    0.0769    0.0545    0.0375    0.0226    \n",
      "plDDT           0.5057    0.2235    0.1387    0.0921    0.0520    0.0308    \n",
      "psi             0.1522    0.0900    0.0687    0.0504    0.0334    0.0238    \n",
      "sasa            0.2744    0.1706    0.1243    0.0889    0.0633    0.0401    \n",
      "ss              0.0158    0.0083    0.0020    0.0017    0.0007    0.0006    \n",
      "tau             0.1648    0.0875    0.0651    0.0466    0.0287    0.0211    \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== ANALYSIS BY WINDOW SIZE ===\n",
      "\n",
      "=== Window Size ±0 ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         RANDOM_FOREST   XGBOOST         LIGHTGBM        GRADIENT_BOOSTING LOGISTIC        SHAP            \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            0.0962       0.0730       247.0000       0.0538       0.0350       0.1645       \n",
      "chi2            0.0940       0.0760       330.0000       0.0622       0.0161       0.1701       \n",
      "chi3            0.0928       0.0722       325.0000       0.0539       0.0047       0.1760       \n",
      "chi4            0.0951       0.0748       292.0000       0.0683       0.0079       0.1846       \n",
      "omega           0.0953       0.0799       323.0000       0.0824       0.0273       0.1747       \n",
      "phi             0.0902       0.0720       301.0000       0.0362       0.0058       0.1594       \n",
      "plDDT           0.1295       0.1209       307.0000       0.4276       0.5774       0.5057       \n",
      "psi             0.0894       0.0720       270.0000       0.0401       0.0116       0.1522       \n",
      "sasa            0.1049       0.0833       342.0000       0.1482       0.2399       0.2744       \n",
      "ss              0.0064       0.0688       6.6667       0.0007       0.0155       0.0158       \n",
      "tau             0.0932       0.0697       243.0000       0.0253       0.0491       0.1648       \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Window Size ±1 ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         RANDOM_FOREST   XGBOOST         LIGHTGBM        GRADIENT_BOOSTING LOGISTIC        SHAP            \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            0.0321       0.0218       102.6667       0.0178       0.0253       0.0899       \n",
      "chi2            0.0316       0.0216       124.3333       0.0261       0.0219       0.1201       \n",
      "chi3            0.0253       0.0228       69.3333       0.0117       0.0305       0.0785       \n",
      "chi4            0.0199       0.0543       87.0000       0.0688       0.0507       0.1459       \n",
      "omega           0.0338       0.0219       110.0000       0.0143       0.0187       0.1011       \n",
      "phi             0.0330       0.0222       106.6667       0.0154       0.0459       0.1147       \n",
      "plDDT           0.0458       0.0336       91.0000       0.0968       0.1916       0.2235       \n",
      "psi             0.0318       0.0217       89.3333       0.0114       0.0252       0.0900       \n",
      "sasa            0.0418       0.0274       119.3333       0.0591       0.2230       0.1706       \n",
      "ss              0.0021       0.0217       3.0000       0.0002       0.0629       0.0083       \n",
      "tau             0.0318       0.0209       91.3333       0.0115       0.0683       0.0875       \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Window Size ±2 ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         RANDOM_FOREST   XGBOOST         LIGHTGBM        GRADIENT_BOOSTING LOGISTIC        SHAP            \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            0.0199       0.0149       62.2000       0.0095       0.0283       0.0682       \n",
      "chi2            0.0195       0.0151       71.6000       0.0193       0.0237       0.1042       \n",
      "chi3            0.0142       0.0174       36.0000       0.0099       0.0273       0.0533       \n",
      "chi4            0.0113       0.0303       51.8000       0.0455       0.0449       0.1067       \n",
      "omega           0.0203       0.0142       63.0000       0.0068       0.0149       0.0743       \n",
      "phi             0.0200       0.0144       63.0000       0.0091       0.0335       0.0769       \n",
      "plDDT           0.0273       0.0256       50.8000       0.0498       0.1850       0.1387       \n",
      "psi             0.0193       0.0142       57.6000       0.0074       0.0424       0.0687       \n",
      "sasa            0.0249       0.0174       86.8000       0.0362       0.1610       0.1243       \n",
      "ss              0.0013       0.0071       1.4000       0.0003       0.0292       0.0020       \n",
      "tau             0.0196       0.0153       53.0000       0.0054       0.0497       0.0651       \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Window Size ±4 ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         RANDOM_FOREST   XGBOOST         LIGHTGBM        GRADIENT_BOOSTING LOGISTIC        SHAP            \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            0.0110       0.0082       35.0000       0.0048       0.0209       0.0513       \n",
      "chi2            0.0111       0.0087       46.6667       0.0104       0.0312       0.0765       \n",
      "chi3            0.0079       0.0106       24.4444       0.0069       0.0344       0.0430       \n",
      "chi4            0.0056       0.0182       29.0000       0.0263       0.0733       0.0688       \n",
      "omega           0.0113       0.0076       32.7778       0.0032       0.0196       0.0522       \n",
      "phi             0.0110       0.0074       36.1111       0.0052       0.0297       0.0545       \n",
      "plDDT           0.0156       0.0138       24.0000       0.0287       0.1751       0.0921       \n",
      "psi             0.0107       0.0076       31.1111       0.0036       0.0494       0.0504       \n",
      "sasa            0.0140       0.0098       44.4444       0.0189       0.1052       0.0889       \n",
      "ss              0.0008       0.0037       0.7037       0.0001       0.0378       0.0017       \n",
      "tau             0.0108       0.0080       27.6667       0.0029       0.0416       0.0466       \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Window Size ±8 ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         RANDOM_FOREST   XGBOOST         LIGHTGBM        GRADIENT_BOOSTING LOGISTIC        SHAP            \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            0.0060       0.0045       17.1176       0.0022       0.0205       0.0337       \n",
      "chi2            0.0057       0.0046       23.3529       0.0047       0.0353       0.0509       \n",
      "chi3            0.0042       0.0053       13.5882       0.0037       0.0321       0.0285       \n",
      "chi4            0.0029       0.0082       18.9412       0.0155       0.0728       0.0581       \n",
      "omega           0.0061       0.0042       18.5294       0.0023       0.0270       0.0367       \n",
      "phi             0.0059       0.0042       18.4706       0.0024       0.0273       0.0375       \n",
      "plDDT           0.0083       0.0089       12.4118       0.0146       0.1561       0.0520       \n",
      "psi             0.0057       0.0047       13.5294       0.0018       0.0460       0.0334       \n",
      "sasa            0.0071       0.0055       26.1765       0.0094       0.0991       0.0633       \n",
      "ss              0.0004       0.0014       0.2353       0.0000       0.0555       0.0007       \n",
      "tau             0.0058       0.0045       13.6471       0.0021       0.0526       0.0287       \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Window Size ±16 ===\n",
      "--------------------------------------------------------------------------------\n",
      "Feature         RANDOM_FOREST   XGBOOST         LIGHTGBM        GRADIENT_BOOSTING LOGISTIC        SHAP            \n",
      "--------------------------------------------------------------------------------\n",
      "chi1            0.0030       0.0024       8.6061       0.0017       0.0191       0.0210       \n",
      "chi2            0.0030       0.0025       11.7879       0.0021       0.0309       0.0320       \n",
      "chi3            0.0021       0.0028       6.8182       0.0019       0.0355       0.0168       \n",
      "chi4            0.0013       0.0035       9.6061       0.0075       0.0702       0.0338       \n",
      "omega           0.0032       0.0023       8.2424       0.0010       0.0227       0.0231       \n",
      "phi             0.0032       0.0024       10.1212       0.0011       0.0265       0.0226       \n",
      "plDDT           0.0043       0.0048       6.2727       0.0075       0.1437       0.0308       \n",
      "psi             0.0030       0.0023       6.7273       0.0009       0.0375       0.0238       \n",
      "sasa            0.0036       0.0029       13.7273       0.0056       0.0610       0.0401       \n",
      "ss              0.0002       0.0007       0.1313       0.0000       0.0660       0.0006       \n",
      "tau             0.0030       0.0024       8.6061       0.0009       0.0444       0.0211       \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "class FeatureImportanceAnalyzer:\n",
    "    def __init__(self, continuous_features, discrete_features):\n",
    "        self.continuous_features = continuous_features\n",
    "        self.discrete_features = discrete_features\n",
    "        self.models = {\n",
    "            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'xgboost': xgb.XGBClassifier(n_estimators=100, random_state=42),\n",
    "            'lightgbm': lgb.LGBMClassifier(n_estimators=100, random_state=42),\n",
    "            'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "            'logistic': LogisticRegression(max_iter=1000, random_state=42)\n",
    "        }\n",
    "        \n",
    "    def prepare_data(self, df, window_size, middle_pos=16):\n",
    "        X_processed = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # Process continuous features\n",
    "        scaler = StandardScaler()\n",
    "        for feature in self.continuous_features:\n",
    "            positions = list(range(middle_pos-window_size, middle_pos+window_size+1))\n",
    "            for pos in positions:\n",
    "                values = np.array([x[pos] for x in df[feature].values])\n",
    "                mask = values != 0\n",
    "                if mask.any():\n",
    "                    non_zero_values = values[mask].reshape(-1, 1)\n",
    "                    values[mask] = scaler.fit_transform(non_zero_values).flatten()\n",
    "                X_processed.append(values)\n",
    "                feature_names.append(f\"{feature}_{pos-middle_pos}\")\n",
    "        \n",
    "        # Process SS\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        positions = list(range(middle_pos-window_size, middle_pos+window_size+1))\n",
    "        for pos in positions:\n",
    "            ss_values = np.array([x[pos] for x in df['ss'].values]).reshape(-1, 1)\n",
    "            encoded_ss = encoder.fit_transform(ss_values)\n",
    "            for i, ss_type in enumerate(encoder.categories_[0]):\n",
    "                X_processed.append(encoded_ss[:, i])\n",
    "                feature_names.append(f\"ss_{ss_type}_{pos-middle_pos}\")\n",
    "        \n",
    "        X = np.array(X_processed).T\n",
    "        y = df['label'].values\n",
    "        \n",
    "        return X, y, feature_names\n",
    "    \n",
    "    def get_model_importance(self, model_name, X, y, feature_names):\n",
    "        model = self.models[model_name]\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        if model_name == 'logistic':\n",
    "            importance = np.abs(model.coef_[0])\n",
    "        else:\n",
    "            importance = model.feature_importances_\n",
    "            \n",
    "        return importance\n",
    "    \n",
    "    def get_shap_importance(self, X, y, feature_names):\n",
    "        model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0]\n",
    "        \n",
    "        return np.abs(shap_values).mean(axis=0)\n",
    "    \n",
    "    def analyze_all_models(self, df, window_size):\n",
    "        X, y, feature_names = self.prepare_data(df, window_size)\n",
    "        \n",
    "        results = {}\n",
    "        # Get importance from each model\n",
    "        for model_name in self.models.keys():\n",
    "            importance = self.get_model_importance(model_name, X, y, feature_names)\n",
    "            results[model_name] = self.aggregate_importance(importance, feature_names)\n",
    "            \n",
    "        # Add SHAP importance\n",
    "        try:\n",
    "            shap_importance = self.get_shap_importance(X, y, feature_names)\n",
    "            results['shap'] = self.aggregate_importance(shap_importance, feature_names)\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP analysis failed: {e}\")\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def aggregate_importance(self, importance, feature_names):\n",
    "        feature_importance = {}\n",
    "        for feat_type in self.continuous_features + ['ss']:\n",
    "            if feat_type == 'ss':\n",
    "                mask = [f.startswith('ss_') for f in feature_names]\n",
    "            else:\n",
    "                mask = [f.startswith(feat_type + '_') for f in feature_names]\n",
    "            importance_values = importance[mask]\n",
    "            feature_importance[feat_type] = np.mean(importance_values)\n",
    "        return feature_importance\n",
    "\n",
    "def print_model_importance_tables(all_results):\n",
    "    \"\"\"Print feature importance table for each model\"\"\"\n",
    "    model_names = list(all_results[0].keys())  # Get all model names\n",
    "    window_sizes = list(all_results.keys())    # Get all window sizes\n",
    "    \n",
    "    for model in model_names:\n",
    "        print(f\"\\n=== {model.upper()} ===\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Print header with window sizes\n",
    "        print(f\"{'Feature':<15} \", end=\"\")\n",
    "        for size in window_sizes:\n",
    "            print(f\"Win±{size:<12} \", end=\"\")\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        \n",
    "        # Get all features for this model\n",
    "        features = list(all_results[0][model].keys())\n",
    "        \n",
    "        # Print each feature's importance across window sizes\n",
    "        for feature in sorted(features):\n",
    "            print(f\"{feature:<15} \", end=\"\")\n",
    "            for size in window_sizes:\n",
    "                importance = all_results[size][model].get(feature, 0)\n",
    "                print(f\"{importance:,.4f}    \", end=\"\")\n",
    "            print()\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def print_window_importance_tables(all_results):\n",
    "    \"\"\"Print feature importance table for each window size\"\"\"\n",
    "    for size in all_results.keys():\n",
    "        print(f\"\\n=== Window Size ±{size} ===\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Print header\n",
    "        print(f\"{'Feature':<15} \", end=\"\")\n",
    "        model_names = list(all_results[size].keys())\n",
    "        for model in model_names:\n",
    "            print(f\"{model.upper():<15} \", end=\"\")\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        \n",
    "        # Get all features\n",
    "        features = list(all_results[size][list(all_results[size].keys())[0]].keys())\n",
    "        \n",
    "        # Print each feature's importance across models\n",
    "        for feature in sorted(features):\n",
    "            print(f\"{feature:<15} \", end=\"\")\n",
    "            for model in model_names:\n",
    "                importance = all_results[size][model].get(feature, 0)\n",
    "                print(f\"{importance:,.4f}       \", end=\"\")\n",
    "            print()\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def analyze_features(train_df_path):\n",
    "    # Define features\n",
    "    continuous_features = [\n",
    "        'phi', 'psi', 'omega', 'tau', \n",
    "        'chi1', 'chi2', 'chi3', 'chi4',\n",
    "        'sasa', 'plDDT'\n",
    "    ]\n",
    "    discrete_features = ['ss']\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = FeatureImportanceAnalyzer(continuous_features, discrete_features)\n",
    "    \n",
    "    # Read data\n",
    "    train_df_path = os.path.abspath(os.path.expanduser(train_df_path))\n",
    "    print(f\"Reading data from: {train_df_path}\")\n",
    "    train_df = pd.read_csv(train_df_path)\n",
    "    \n",
    "    # Process features\n",
    "    processed_df = pd.DataFrame()\n",
    "    processed_df['label'] = train_df['label']\n",
    "    \n",
    "    def process_feature(df, feature_name):\n",
    "        if feature_name == 'ss':\n",
    "            return df[feature_name].apply(list)\n",
    "        else:\n",
    "            return df[feature_name].apply(lambda x: \n",
    "                np.array([float(v.strip()) for v in x.strip('[]').split(',')]))\n",
    "    \n",
    "    for feature in continuous_features:\n",
    "        processed_df[feature] = process_feature(train_df, feature)\n",
    "    processed_df['ss'] = train_df['ss'].apply(list)\n",
    "    \n",
    "    # Analyze with different window sizes\n",
    "    window_sizes = [0, 1, 2, 4, 8, 16]\n",
    "    all_results = {}\n",
    "    \n",
    "    for size in window_sizes:\n",
    "        print(f\"Processing window size ±{size}\")\n",
    "        results = analyzer.analyze_all_models(processed_df, size)\n",
    "        all_results[size] = results\n",
    "    \n",
    "    # Print both views of feature importance\n",
    "    print(\"\\n=== ANALYSIS BY MODEL ===\")\n",
    "    print_model_importance_tables(all_results)\n",
    "    \n",
    "    print(\"\\n=== ANALYSIS BY WINDOW SIZE ===\")\n",
    "    print_window_importance_tables(all_results)\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = 'feature_importance_results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Create comparison visualizations\n",
    "    model_names = list(analyzer.models.keys()) + ['shap']\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        try:\n",
    "            results_df = pd.DataFrame({\n",
    "                size: results[model_name] \n",
    "                for size, results in all_results.items()\n",
    "            })\n",
    "            results_df.columns = [f'±{size}' for size in window_sizes]\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(results_df, annot=True, fmt='.3f', cmap='YlOrRd')\n",
    "            plt.title(f'Feature Importance vs Window Size ({model_name.upper()})')\n",
    "            plt.xlabel('Window Size')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(results_dir, f'feature_importance_heatmap_{model_name}.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Save numerical results\n",
    "            results_df.to_csv(os.path.join(results_dir, f'feature_importance_{model_name}.csv'))\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"Skipping {model_name} visualization due to missing results\")\n",
    "            continue\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    results = analyze_features('../../data/train/structure/processed_features_train.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-dephos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
