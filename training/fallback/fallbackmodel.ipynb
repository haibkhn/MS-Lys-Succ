{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 11:50:21.922262: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-13 11:50:23.080799: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/:\n",
      "2025-04-13 11:50:23.080913: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/:\n",
      "2025-04-13 11:50:23.080922: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data identifiers...\n",
      "Loading structure index from: ../data/processed_features_fixed_train_contactmap.csv...\n",
      "\n",
      "Processing positive data...\n",
      "Processing negative data...\n",
      "\n",
      "Warning: Found 435 duplicate entries based on ('entry', 'pos').\n",
      "\n",
      "Dataset statistics:\n",
      "Total entries loaded: 9500\n",
      "Positive examples: 4750\n",
      "Negative examples: 4750\n",
      "Entries marked as having structure: 8853\n",
      "Unique proteins (entries): 2193\n",
      "\n",
      "Loading test data identifiers...\n",
      "Loading structure index from: ../data/processed_features_fixed_test_contactmap.csv...\n",
      "\n",
      "Processing positive data...\n",
      "Processing negative data...\n",
      "\n",
      "Dataset statistics:\n",
      "Total entries loaded: 3224\n",
      "Positive examples: 253\n",
      "Negative examples: 2971\n",
      "Entries marked as having structure: 2737\n",
      "Unique proteins (entries): 123\n",
      "\n",
      "Loading ProtT5 embeddings...\n",
      "Aligning training data with ProtT5...\n",
      "Aligning test data with ProtT5...\n",
      "\n",
      "Aligned training data size: 9500\n",
      "Aligned test data size: 3224\n",
      "\n",
      "Preparing full training data...\n",
      "Using Sequence Length: 33, Alphabet Size: 21, ProtT5 Dim: 1024\n",
      "Full training data shapes:\n",
      "X_train_seq_all: (9500, 33)\n",
      "X_train_prot_t5: (9500, 1024)\n",
      "y_train_all: (9500,)\n",
      "\n",
      "Preparing test data subset (no structure)...\n",
      "Test data subset (no structure) shapes:\n",
      "X_test_seq_no_struct: (487, 33)\n",
      "X_test_prot_t5_no_struct: (487, 1024)\n",
      "y_test_no_struct: (487,)\n",
      "Number of test samples without structure: 487\n",
      "Positive samples in this subset: 13\n",
      "Negative samples in this subset: 474\n",
      "\n",
      "Class weights (full training set): {0: 1.0, 1: 1.0}\n",
      "\n",
      "===== FOLD 1/5 =====\n",
      "\n",
      "--- Training Sequence Only ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 11:50:33.822551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-13 11:50:33.863091: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/:\n",
      "2025-04-13 11:50:33.863131: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-04-13 11:50:33.864352: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Epoch 15: early stopping\n",
      "60/60 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Validation Metrics (Fold 1) - Sequence Only:  Acc: 0.7489, BAcc: 0.7483, MCC: 0.4996\n",
      "\n",
      "--- Training ProtT5 Only ---\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 9: early stopping\n",
      "60/60 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Validation Metrics (Fold 1) - ProtT5 Only:  Acc: 0.7411, BAcc: 0.7402, MCC: 0.4852\n",
      "\n",
      "--- Training Sequence + ProtT5 ---\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 30: early stopping\n",
      "60/60 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "Validation Metrics (Fold 1) - Sequence + ProtT5:  Acc: 0.7726, BAcc: 0.7718, MCC: 0.5486\n",
      "\n",
      "===== FOLD 2/5 =====\n",
      "\n",
      "--- Training Sequence Only ---\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Epoch 14: early stopping\n",
      "60/60 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Validation Metrics (Fold 2) - Sequence Only:  Acc: 0.7389, BAcc: 0.7385, MCC: 0.4777\n",
      "\n",
      "--- Training ProtT5 Only ---\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 9: early stopping\n",
      "60/60 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Validation Metrics (Fold 2) - ProtT5 Only:  Acc: 0.7289, BAcc: 0.7273, MCC: 0.4625\n",
      "\n",
      "--- Training Sequence + ProtT5 ---\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Epoch 29: early stopping\n",
      "60/60 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Validation Metrics (Fold 2) - Sequence + ProtT5:  Acc: 0.7853, BAcc: 0.7846, MCC: 0.5711\n",
      "\n",
      "===== FOLD 3/5 =====\n",
      "\n",
      "--- Training Sequence Only ---\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Epoch 13: early stopping\n",
      "60/60 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Validation Metrics (Fold 3) - Sequence Only:  Acc: 0.7347, BAcc: 0.7365, MCC: 0.4729\n",
      "\n",
      "--- Training ProtT5 Only ---\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 10: early stopping\n",
      "60/60 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Validation Metrics (Fold 3) - ProtT5 Only:  Acc: 0.7032, BAcc: 0.7067, MCC: 0.4155\n",
      "\n",
      "--- Training Sequence + ProtT5 ---\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 30: early stopping\n",
      "60/60 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "Validation Metrics (Fold 3) - Sequence + ProtT5:  Acc: 0.7647, BAcc: 0.7670, MCC: 0.5342\n",
      "\n",
      "===== FOLD 4/5 =====\n",
      "\n",
      "--- Training Sequence Only ---\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 16: early stopping\n",
      "60/60 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Validation Metrics (Fold 4) - Sequence Only:  Acc: 0.7363, BAcc: 0.7354, MCC: 0.4723\n",
      "\n",
      "--- Training ProtT5 Only ---\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 11: early stopping\n",
      "60/60 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Validation Metrics (Fold 4) - ProtT5 Only:  Acc: 0.7432, BAcc: 0.7409, MCC: 0.4900\n",
      "\n",
      "--- Training Sequence + ProtT5 ---\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "Epoch 33: early stopping\n",
      "60/60 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Validation Metrics (Fold 4) - Sequence + ProtT5:  Acc: 0.7874, BAcc: 0.7867, MCC: 0.5744\n",
      "\n",
      "===== FOLD 5/5 =====\n",
      "\n",
      "--- Training Sequence Only ---\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Epoch 14: early stopping\n",
      "60/60 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Validation Metrics (Fold 5) - Sequence Only:  Acc: 0.7521, BAcc: 0.7524, MCC: 0.5074\n",
      "\n",
      "--- Training ProtT5 Only ---\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 11: early stopping\n",
      "60/60 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Validation Metrics (Fold 5) - ProtT5 Only:  Acc: 0.7316, BAcc: 0.7318, MCC: 0.4653\n",
      "\n",
      "--- Training Sequence + ProtT5 ---\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 30: early stopping\n",
      "60/60 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Validation Metrics (Fold 5) - Sequence + ProtT5:  Acc: 0.7853, BAcc: 0.7854, MCC: 0.5714\n",
      "\n",
      "===== FINAL EVALUATION =====\n",
      "\n",
      "--- Results Summary for: Sequence Only ---\n",
      "\n",
      "Average Cross-validation Results (Validation Set):\n",
      "ACC: 0.7422 ± 0.0070\n",
      "BALANCED_ACC: 0.7422 ± 0.0068\n",
      "MCC: 0.4860 ± 0.0146\n",
      "SN: 0.7817 ± 0.0172\n",
      "SP: 0.7028 ± 0.0067\n",
      "\n",
      "Final Test Set Results (No Structure Subset):\n",
      "ACC: 0.6838\n",
      "BALANCED_ACC: 0.7253\n",
      "MCC: 0.1544\n",
      "SN: 0.7692\n",
      "SP: 0.6814\n",
      "Confusion Matrix (Test Set):\n",
      "[[323 151]\n",
      " [  3  10]]\n",
      "--------------------------------------\n",
      "\n",
      "--- Results Summary for: ProtT5 Only ---\n",
      "\n",
      "Average Cross-validation Results (Validation Set):\n",
      "ACC: 0.7296 ± 0.0143\n",
      "BALANCED_ACC: 0.7294 ± 0.0125\n",
      "MCC: 0.4637 ± 0.0264\n",
      "SN: 0.7999 ± 0.0216\n",
      "SP: 0.6588 ± 0.0198\n",
      "\n",
      "Final Test Set Results (No Structure Subset):\n",
      "ACC: 0.8665\n",
      "BALANCED_ACC: 0.8192\n",
      "MCC: 0.2899\n",
      "SN: 0.7692\n",
      "SP: 0.8692\n",
      "Confusion Matrix (Test Set):\n",
      "[[412  62]\n",
      " [  3  10]]\n",
      "------------------------------------\n",
      "\n",
      "--- Results Summary for: Sequence + ProtT5 ---\n",
      "\n",
      "Average Cross-validation Results (Validation Set):\n",
      "ACC: 0.7791 ± 0.0089\n",
      "BALANCED_ACC: 0.7791 ± 0.0081\n",
      "MCC: 0.5599 ± 0.0159\n",
      "SN: 0.8176 ± 0.0101\n",
      "SP: 0.7406 ± 0.0215\n",
      "\n",
      "Final Test Set Results (No Structure Subset):\n",
      "ACC: 0.8789\n",
      "BALANCED_ACC: 0.8255\n",
      "MCC: 0.3066\n",
      "SN: 0.7692\n",
      "SP: 0.8819\n",
      "Confusion Matrix (Test Set):\n",
      "[[418  56]\n",
      " [  3  10]]\n",
      "------------------------------------------\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler # Keep for potential use in prepare_structure_data if needed elsewhere, but not directly used here\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gc # Garbage Collection\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# --- Data Loading and Preparation Functions ---\n",
    "def load_prot_t5_data(pos_file, neg_file):\n",
    "    \"\"\"Load ProtT5 embeddings and align with existing data\"\"\"\n",
    "    # Read positive and negative files\n",
    "    pos_data = []\n",
    "    with open(pos_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            entry = parts[0]\n",
    "            pos = int(parts[1])\n",
    "            embeddings = [float(x) for x in parts[2:]]\n",
    "            pos_data.append((entry, pos, embeddings))\n",
    "\n",
    "    neg_data = []\n",
    "    with open(neg_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            entry = parts[0]\n",
    "            pos = int(parts[1])\n",
    "            embeddings = [float(x) for x in parts[2:]]\n",
    "            neg_data.append((entry, pos, embeddings))\n",
    "\n",
    "    # Convert to dictionaries for easy lookup\n",
    "    pos_dict = {(entry, pos): emb for entry, pos, emb in pos_data}\n",
    "    neg_dict = {(entry, pos): emb for entry, pos, emb in neg_data}\n",
    "\n",
    "    return pos_dict, neg_dict\n",
    "\n",
    "def prepare_aligned_data(seq_struct_df, pos_dict, neg_dict):\n",
    "    \"\"\"Align ProtT5 embeddings with sequence+structure data\"\"\"\n",
    "    embeddings = []\n",
    "    aligned_indices = []\n",
    "    original_indices_map = {} # Map new index to original index\n",
    "\n",
    "    for i, (idx, row) in enumerate(seq_struct_df.iterrows()):\n",
    "        key = (row['entry'], row['pos'])\n",
    "        emb = pos_dict.get(key) if row['label'] == 1 else neg_dict.get(key)\n",
    "\n",
    "        if emb is not None:\n",
    "            embeddings.append(emb)\n",
    "            aligned_indices.append(idx)\n",
    "            original_indices_map[i] = idx # Store mapping: new index -> original df index\n",
    "\n",
    "    # Convert to numpy array\n",
    "    X_prot_t5 = np.array(embeddings)\n",
    "\n",
    "    # Get aligned sequence+structure data\n",
    "    aligned_df = seq_struct_df.loc[aligned_indices].reset_index(drop=True) # Reset index for easier mapping\n",
    "\n",
    "    return X_prot_t5, aligned_df\n",
    "\n",
    "def extract_entry_id(header):\n",
    "    \"\"\"Extract entry ID between first and second '|' characters if present, otherwise return as is\"\"\"\n",
    "    if '|' in header:\n",
    "        try:\n",
    "            return header.split('|')[1]\n",
    "        except IndexError: # Handle cases like '>|P12345|-' where split creates ['', 'P12345', '-']\n",
    "             if len(header.split('|')) > 1:\n",
    "                 return header.split('|')[1]\n",
    "             else:\n",
    "                 print(f\"Warning: Could not parse header with pipes: {header}\")\n",
    "                 return header\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not parse header '{header}': {e}\")\n",
    "            return header\n",
    "    else:\n",
    "        # If no pipes, assume it's already an ID\n",
    "        return header\n",
    "\n",
    "def load_complete_data(mode='train', exclude_emb=False, base_data_path='../../data', struct_path_override=None):\n",
    "    \"\"\"\n",
    "    Load data line by line ensuring perfect matching between FASTA and ProtT5 files\n",
    "    Uses the 'has_structure' flag derived from the structure file.\n",
    "    \"\"\"\n",
    "    # Set paths based on mode\n",
    "    if mode == 'train':\n",
    "        pos_fasta = f'{base_data_path}/train/fasta/positive_sites.fasta'\n",
    "        neg_fasta = f'{base_data_path}/train/fasta/negative_sites.fasta'\n",
    "        # ProtT5 paths are handled later by load_prot_t5_data\n",
    "        struct_path = struct_path_override if struct_path_override else f\"{base_data_path}/train/structure/processed_features_train.csv\" # Adjust relative path\n",
    "    else:  # test\n",
    "        pos_fasta = f'{base_data_path}/test/fasta/test_positive_sites.fasta'\n",
    "        neg_fasta = f'{base_data_path}/test/fasta/test_negative_sites.fasta'\n",
    "        # ProtT5 paths are handled later by load_prot_t5_data\n",
    "        struct_path = struct_path_override if struct_path_override else f\"{base_data_path}/test/structure/processed_features_test.csv\" # Adjust relative path\n",
    "\n",
    "    print(f\"Loading structure index from: {struct_path}...\")\n",
    "    try:\n",
    "        struct_data = pd.read_csv(struct_path)\n",
    "         # Clean structure data entry IDs only if 'entry' column exists\n",
    "        if 'entry' in struct_data.columns:\n",
    "            struct_data['entry'] = struct_data['entry'].apply(lambda x: extract_entry_id(x) if isinstance(x, str) else x)\n",
    "        else:\n",
    "            print(\"Warning: 'entry' column not found in structure file. Cannot clean IDs.\")\n",
    "            # Handle cases where entry might be in another column or format\n",
    "            # Example: Assuming the first column might contain the ID if 'entry' is missing\n",
    "            if len(struct_data.columns) > 0 and 'pos' in struct_data.columns:\n",
    "                 id_col = struct_data.columns[0]\n",
    "                 print(f\"Attempting to use column '{id_col}' as entry ID.\")\n",
    "                 struct_data['entry'] = struct_data[id_col].apply(lambda x: extract_entry_id(x) if isinstance(x, str) else x)\n",
    "            else:\n",
    "                 print(\"Error: Cannot determine entry ID column in structure file.\")\n",
    "                 struct_data = pd.DataFrame(columns=['entry', 'pos']) # Create empty df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Structure file not found at {struct_path}. Assuming no structure data available.\")\n",
    "        struct_data = pd.DataFrame(columns=['entry', 'pos']) # Create empty df to avoid errors\n",
    "\n",
    "    # Create dictionary for quick lookup of structure data availability\n",
    "    struct_dict_keys = set()\n",
    "    if 'entry' in struct_data.columns and 'pos' in struct_data.columns:\n",
    "        for _, row in struct_data.iterrows():\n",
    "            # Ensure entry is not NaN or None before creating the key\n",
    "            if pd.notna(row['entry']) and pd.notna(row['pos']):\n",
    "                 key = (str(row['entry']), int(row['pos'])) # Ensure consistent types\n",
    "                 struct_dict_keys.add(key)\n",
    "            else:\n",
    "                 print(f\"Warning: Skipping row with missing entry/pos in structure file: {row}\")\n",
    "    else:\n",
    "         print(\"Warning: 'entry' or 'pos' column missing in structure data. Cannot determine structure availability accurately.\")\n",
    "\n",
    "\n",
    "    print(\"\\nProcessing positive data...\")\n",
    "    positive_data = []\n",
    "\n",
    "    # Process positive data\n",
    "    try:\n",
    "        with open(pos_fasta) as fasta_file:\n",
    "            fasta_lines = fasta_file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Positive FASTA file not found at {pos_fasta}\")\n",
    "        fasta_lines = []\n",
    "\n",
    "    for i in range(0, len(fasta_lines), 2):\n",
    "        # Process FASTA header\n",
    "        header = fasta_lines[i].strip()[1:]  # remove '>'\n",
    "        sequence = fasta_lines[i + 1].strip()\n",
    "\n",
    "        try:\n",
    "            header_parts = header.split('|-|')\n",
    "            entry = extract_entry_id(header_parts[0])\n",
    "            pos = int(header_parts[1])\n",
    "        except (IndexError, ValueError):\n",
    "            print(f\"Warning: Skipping malformed positive FASTA header: {header}\")\n",
    "            continue\n",
    "\n",
    "        # Create data entry\n",
    "        data_entry = {\n",
    "            'entry': str(entry), # Ensure consistent type\n",
    "            'pos': pos,\n",
    "            'sequence': sequence,\n",
    "            'label': 1,\n",
    "            'has_structure': False\n",
    "        }\n",
    "\n",
    "        # Check if structure features are available\n",
    "        if (data_entry['entry'], data_entry['pos']) in struct_dict_keys:\n",
    "            data_entry['has_structure'] = True\n",
    "\n",
    "        positive_data.append(data_entry)\n",
    "\n",
    "    print(\"Processing negative data...\")\n",
    "    negative_data = []\n",
    "\n",
    "    # Process negative data\n",
    "    try:\n",
    "        with open(neg_fasta) as fasta_file:\n",
    "            fasta_lines = fasta_file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Negative FASTA file not found at {neg_fasta}\")\n",
    "        fasta_lines = []\n",
    "\n",
    "    for i in range(0, len(fasta_lines), 2):\n",
    "        # Process FASTA header\n",
    "        header = fasta_lines[i].strip()[1:]  # remove '>'\n",
    "        sequence = fasta_lines[i + 1].strip()\n",
    "\n",
    "        try:\n",
    "            header_parts = header.split('|-|')\n",
    "            entry = extract_entry_id(header_parts[0])\n",
    "            pos = int(header_parts[1])\n",
    "        except (IndexError, ValueError):\n",
    "            print(f\"Warning: Skipping malformed negative FASTA header: {header}\")\n",
    "            continue\n",
    "\n",
    "        # Create data entry\n",
    "        data_entry = {\n",
    "            'entry': str(entry), # Ensure consistent type\n",
    "            'pos': pos,\n",
    "            'sequence': sequence,\n",
    "            'label': 0,\n",
    "            'has_structure': False\n",
    "        }\n",
    "\n",
    "        # Check if structure features are available\n",
    "        if (data_entry['entry'], data_entry['pos']) in struct_dict_keys:\n",
    "            data_entry['has_structure'] = True\n",
    "\n",
    "        negative_data.append(data_entry)\n",
    "\n",
    "    # Combine all data\n",
    "    if not positive_data and not negative_data:\n",
    "         print(\"Error: No data loaded from FASTA files. Returning empty DataFrame.\")\n",
    "         return pd.DataFrame() # Return empty if no data\n",
    "\n",
    "    all_data = pd.DataFrame(positive_data + negative_data)\n",
    "\n",
    "    # Check for duplicates\n",
    "    duplicates = all_data.duplicated(subset=['entry', 'pos'], keep=False)\n",
    "    if duplicates.any():\n",
    "       print(f\"\\nWarning: Found {duplicates.sum()} duplicate entries based on ('entry', 'pos').\")\n",
    "       # print(all_data[duplicates][['entry', 'pos', 'label']].sort_values(['entry', 'pos']))\n",
    "       # Decide on handling duplicates if necessary (e.g., remove them)\n",
    "       # all_data = all_data.drop_duplicates(subset=['entry', 'pos'], keep='first')\n",
    "       # print(\"Duplicates removed, keeping first occurrence.\")\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\nDataset statistics:\")\n",
    "    print(f\"Total entries loaded: {len(all_data)}\")\n",
    "    if not all_data.empty:\n",
    "        print(f\"Positive examples: {all_data['label'].sum()}\")\n",
    "        print(f\"Negative examples: {len(all_data) - all_data['label'].sum()}\")\n",
    "        print(f\"Entries marked as having structure: {all_data['has_structure'].sum()}\")\n",
    "        print(f\"Unique proteins (entries): {all_data['entry'].nunique()}\")\n",
    "    else:\n",
    "        print(\"No data to calculate statistics.\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def prepare_sequence_data(df):\n",
    "    \"\"\"Convert sequences to integer encoding\"\"\"\n",
    "    # alphabet = 'ARNDCQEGHILKMFPSTWYV-' # Include gap character if needed\n",
    "    alphabet = 'ARNDCQEGHILKMFPSTWYV'  # Standard 20 amino acids\n",
    "    # Add 'X' for unknown and '-' for gap/padding if present in sequences\n",
    "    valid_chars = set(alphabet)\n",
    "    char_to_int = {c: i for i, c in enumerate(alphabet)}\n",
    "    unknown_int = len(alphabet) # Assign next integer to unknown/gap\n",
    "    char_to_int['-'] = unknown_int\n",
    "    char_to_int['X'] = unknown_int\n",
    "    alphabet_size = len(alphabet) + 1 # +1 for the unknown/gap character\n",
    "\n",
    "    sequences = df['sequence'].values\n",
    "    encodings = []\n",
    "\n",
    "    expected_len = 33 # Fixed length for all sequences\n",
    "\n",
    "    for seq in sequences:\n",
    "        if len(seq) != expected_len:\n",
    "             print(f\"Warning: Sequence length mismatch. Expected {expected_len}, got {len(seq)}. Sequence: {seq[:10]}...{seq[-10:]}. Padding/Truncating.\")\n",
    "             # Handle mismatch: Pad or truncate\n",
    "             if len(seq) > expected_len:\n",
    "                 # Truncate from center (assuming window)\n",
    "                 start = (len(seq) - expected_len) // 2\n",
    "                 seq = seq[start : start + expected_len]\n",
    "             else:\n",
    "                 # Pad with '-'\n",
    "                 padding = '-' * (expected_len - len(seq))\n",
    "                 # Decide padding strategy (e.g., center, end)\n",
    "                 pad_before = (expected_len - len(seq)) // 2\n",
    "                 pad_after = expected_len - len(seq) - pad_before\n",
    "                 seq = '-' * pad_before + seq + '-' * pad_after\n",
    "\n",
    "        try:\n",
    "             integer_encoded = [char_to_int.get(char, unknown_int) for char in seq]\n",
    "            #  integer_encoded = [char_to_int[char] for char in seq if char in char_to_int] # Strict: skip invalid chars\n",
    "             if len(integer_encoded) != expected_len:\n",
    "                  # This shouldn't happen with the padding/truncating above, but as a safeguard:\n",
    "                  print(f\"Error after processing: incorrect length {len(integer_encoded)} for sequence {seq}\")\n",
    "                  # Handle error - e.g., skip sequence, use a default encoding\n",
    "                  # Add a default encoding of the correct length (e.g., all unknowns)\n",
    "                  encodings.append([unknown_int] * expected_len)\n",
    "                  continue\n",
    "             encodings.append(integer_encoded)\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Error processing sequence: Invalid character {e} in sequence '{seq}'. Assigning unknown value.\")\n",
    "            # Replace invalid char on the fly or handle as needed\n",
    "            integer_encoded = [char_to_int.get(char, unknown_int) for char in seq]\n",
    "            encodings.append(integer_encoded)\n",
    "        except Exception as e:\n",
    "            print(f\"General error processing sequence: {e} for sequence '{seq}'\")\n",
    "            # Skip or add default encoding\n",
    "            encodings.append([unknown_int] * expected_len)\n",
    "            continue\n",
    "\n",
    "    if not encodings:\n",
    "         print(\"Warning: No sequences were successfully encoded.\")\n",
    "         # Return an empty array with the correct shape if possible, or handle upstream\n",
    "         return np.empty((0, expected_len), dtype=int)\n",
    "\n",
    "    return np.array(encodings)\n",
    "\n",
    "# --- Model Creation Functions ---\n",
    "\n",
    "def create_sequence_model(seq_length=33, alphabet_size=21):\n",
    "    \"\"\"Create CNN model for sequence data\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(seq_length,)),\n",
    "        # Ensure Embedding input_dim matches the alphabet size used in prepare_sequence_data\n",
    "        tf.keras.layers.Embedding(alphabet_size, 21, input_length=seq_length), # output_dim can be adjusted\n",
    "        tf.keras.layers.Reshape((seq_length, 21, 1)),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(17, 3), activation='relu', padding='valid'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_prot_t5_model(embedding_dim=1024):\n",
    "    \"\"\"Create model for ProtT5 embedding vectors\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(embedding_dim,)),\n",
    "        tf.keras.layers.Dense(256),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_seq_prot_t5_model(seq_length=33, embedding_dim=1024, alphabet_size=21):\n",
    "    \"\"\"Create model with sequence and ProtT5 tracks\"\"\"\n",
    "    regularizer = tf.keras.regularizers.l2(0.01)\n",
    "\n",
    "    # Sequence track\n",
    "    seq_input = tf.keras.layers.Input(shape=(seq_length,), name='sequence_input')\n",
    "    # Ensure Embedding input_dim matches the alphabet size\n",
    "    x_seq = tf.keras.layers.Embedding(alphabet_size, 21, input_length=seq_length)(seq_input)\n",
    "    x_seq = tf.keras.layers.Reshape((seq_length, 21, 1))(x_seq)\n",
    "    x_seq = tf.keras.layers.Conv2D(32, kernel_size=(17, 3), activation='relu', padding='valid')(x_seq)\n",
    "    x_seq = tf.keras.layers.BatchNormalization()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(0.4)(x_seq)\n",
    "    x_seq = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x_seq)\n",
    "    x_seq = tf.keras.layers.Flatten()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dense(32, activation='relu',\n",
    "                                 kernel_regularizer=regularizer,\n",
    "                                 name='seq_features')(x_seq)\n",
    "    x_seq = tf.keras.layers.BatchNormalization()(x_seq)\n",
    "    x_seq = tf.keras.layers.Dropout(0.4)(x_seq)\n",
    "\n",
    "    # ProtT5 track\n",
    "    prot_t5_input = tf.keras.layers.Input(shape=(embedding_dim,), name='prot_t5_input')\n",
    "    x_prot_t5 = tf.keras.layers.Dense(256, kernel_regularizer=regularizer)(prot_t5_input)\n",
    "    x_prot_t5 = tf.keras.layers.BatchNormalization()(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dropout(0.5)(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                     kernel_regularizer=regularizer)(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.BatchNormalization()(x_prot_t5)\n",
    "    x_prot_t5 = tf.keras.layers.Dropout(0.5)(x_prot_t5)\n",
    "\n",
    "    # Combine features (simple concatenation, remove weighting for now as requested)\n",
    "    combined = tf.keras.layers.Concatenate()([x_seq, x_prot_t5])\n",
    "\n",
    "    # Final layers with more regularization\n",
    "    x = tf.keras.layers.Dense(64, activation='relu',\n",
    "                            kernel_regularizer=regularizer)(combined)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu',\n",
    "                            kernel_regularizer=regularizer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[seq_input, prot_t5_input], outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def create_callbacks(patience=5, reduce_lr_patience=3):\n",
    "    \"\"\"Creates standard callbacks for training\"\"\"\n",
    "    return [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=reduce_lr_patience,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def calculate_metrics(y_true, y_pred_proba):\n",
    "    \"\"\"Calculates standard binary classification metrics\"\"\"\n",
    "    y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
    "    metrics = {}\n",
    "    metrics['acc'] = accuracy_score(y_true, y_pred_binary)\n",
    "    metrics['balanced_acc'] = balanced_accuracy_score(y_true, y_pred_binary)\n",
    "    metrics['mcc'] = matthews_corrcoef(y_true, y_pred_binary)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    metrics['sn'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    metrics['sp'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    metrics['cm'] = cm\n",
    "    return metrics\n",
    "\n",
    "def print_results_summary(model_name, cv_metrics, test_metrics):\n",
    "    \"\"\"Prints a formatted summary of CV and Test results\"\"\"\n",
    "    print(f\"\\n--- Results Summary for: {model_name} ---\")\n",
    "\n",
    "    print(\"\\nAverage Cross-validation Results (Validation Set):\")\n",
    "    for metric_name in ['acc', 'balanced_acc', 'mcc', 'sn', 'sp']:\n",
    "        mean_val = np.mean(cv_metrics[metric_name])\n",
    "        std_val = np.std(cv_metrics[metric_name])\n",
    "        print(f\"{metric_name.upper()}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "\n",
    "    print(\"\\nFinal Test Set Results (No Structure Subset):\")\n",
    "    for metric_name in ['acc', 'balanced_acc', 'mcc', 'sn', 'sp']:\n",
    "        print(f\"{metric_name.upper()}: {test_metrics[metric_name]:.4f}\")\n",
    "    print(\"Confusion Matrix (Test Set):\")\n",
    "    print(test_metrics['cm'])\n",
    "    print(\"-\" * (len(model_name) + 25))\n",
    "\n",
    "\n",
    "# --- Main Training and Evaluation Function ---\n",
    "\n",
    "def train_evaluate_no_structure_models(n_splits=5, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains and evaluates Sequence-Only, ProtT5-Only, and Seq+ProtT5 models.\n",
    "    Trains on full data, tests ONLY on samples without structure.\n",
    "    \"\"\"\n",
    "    # Define data paths (Adjust as needed)\n",
    "    BASE_DATA_PATH = '../../data'\n",
    "    TRAIN_POS_PROTT5 = f'{BASE_DATA_PATH}/train/PLM/train_positive_ProtT5-XL-UniRef50.csv'\n",
    "    TRAIN_NEG_PROTT5 = f'{BASE_DATA_PATH}/train/PLM/train_negative_ProtT5-XL-UniRef50.csv'\n",
    "    TEST_POS_PROTT5 = f'{BASE_DATA_PATH}/test/PLM/test_positive_ProtT5-XL-UniRef50.csv'\n",
    "    TEST_NEG_PROTT5 = f'{BASE_DATA_PATH}/test/PLM/test_negative_ProtT5-XL-UniRef50.csv'\n",
    "    # Let load_complete_data determine structure paths based on BASE_DATA_PATH, or override here:\n",
    "    STRUCT_PATH_TRAIN = f'{BASE_DATA_PATH}/train/structure/processed_features_train.csv'\n",
    "    STRUCT_PATH_TEST = f'{BASE_DATA_PATH}/test/structure/processed_features_test.csv'\n",
    "\n",
    "\n",
    "    # 1. Load Data (Sequence Info + Structure Flag)\n",
    "    print(\"Loading training data identifiers...\")\n",
    "    train_df_info = load_complete_data(mode='train', base_data_path=BASE_DATA_PATH, struct_path_override=STRUCT_PATH_TRAIN) # , struct_path_override=STRUCT_PATH_TRAIN)\n",
    "    print(\"\\nLoading test data identifiers...\")\n",
    "    test_df_info = load_complete_data(mode='test', base_data_path=BASE_DATA_PATH, struct_path_override=STRUCT_PATH_TEST) # , struct_path_override=STRUCT_PATH_TEST)\n",
    "\n",
    "    if train_df_info.empty or test_df_info.empty:\n",
    "        print(\"Error: Could not load necessary data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 2. Load and Align ProtT5 Embeddings\n",
    "    print(\"\\nLoading ProtT5 embeddings...\")\n",
    "    try:\n",
    "        train_pos_dict, train_neg_dict = load_prot_t5_data(TRAIN_POS_PROTT5, TRAIN_NEG_PROTT5)\n",
    "        test_pos_dict, test_neg_dict = load_prot_t5_data(TEST_POS_PROTT5, TEST_NEG_PROTT5)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading ProtT5 data: {e}. Make sure paths are correct.\")\n",
    "        return\n",
    "\n",
    "    print(\"Aligning training data with ProtT5...\")\n",
    "    X_train_prot_t5, train_data_aligned = prepare_aligned_data(train_df_info, train_pos_dict, train_neg_dict)\n",
    "    print(\"Aligning test data with ProtT5...\")\n",
    "    X_test_prot_t5, test_data_aligned = prepare_aligned_data(test_df_info, test_pos_dict, test_neg_dict)\n",
    "\n",
    "    del train_df_info, test_df_info, train_pos_dict, train_neg_dict, test_pos_dict, test_neg_dict # Free memory\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\nAligned training data size: {len(train_data_aligned)}\")\n",
    "    print(f\"Aligned test data size: {len(test_data_aligned)}\")\n",
    "\n",
    "    if train_data_aligned.empty or test_data_aligned.empty:\n",
    "        print(\"Error: No data after alignment. Check input files and keys ('entry', 'pos').\")\n",
    "        return\n",
    "\n",
    "    # 3. Prepare Full Training Data\n",
    "    print(\"\\nPreparing full training data...\")\n",
    "    X_train_seq_all = prepare_sequence_data(train_data_aligned)\n",
    "    y_train_all = train_data_aligned['label'].values\n",
    "    # X_train_prot_t5 is already prepared\n",
    "\n",
    "    # --- Determine Alphabet Size ---\n",
    "    # Based on the char_to_int mapping in prepare_sequence_data\n",
    "    # Standard 20 + 1 unknown/gap = 21\n",
    "    alphabet_size = 21\n",
    "    seq_length = X_train_seq_all.shape[1] if X_train_seq_all.ndim == 2 and X_train_seq_all.shape[1] > 0 else 33 # Get from data or default\n",
    "    embedding_dim = X_train_prot_t5.shape[1] if X_train_prot_t5.ndim == 2 and X_train_prot_t5.shape[1] > 0 else 1024 # Get from data or default\n",
    "\n",
    "    print(f\"Using Sequence Length: {seq_length}, Alphabet Size: {alphabet_size}, ProtT5 Dim: {embedding_dim}\")\n",
    "\n",
    "\n",
    "    # Shuffle training data consistently\n",
    "    shuffle_idx_all = np.random.RandomState(SEED).permutation(len(y_train_all))\n",
    "    X_train_seq_all = X_train_seq_all[shuffle_idx_all]\n",
    "    X_train_prot_t5 = X_train_prot_t5[shuffle_idx_all]\n",
    "    y_train_all = y_train_all[shuffle_idx_all]\n",
    "\n",
    "    print(\"Full training data shapes:\")\n",
    "    print(f\"X_train_seq_all: {X_train_seq_all.shape}\")\n",
    "    print(f\"X_train_prot_t5: {X_train_prot_t5.shape}\")\n",
    "    print(f\"y_train_all: {y_train_all.shape}\")\n",
    "\n",
    "    # 4. Prepare Test Data (No Structure Subset Only)\n",
    "    print(\"\\nPreparing test data subset (no structure)...\")\n",
    "    no_struct_mask_test = ~test_data_aligned['has_structure'].values\n",
    "    test_data_no_struct = test_data_aligned[no_struct_mask_test].reset_index(drop=True)\n",
    "\n",
    "    if test_data_no_struct.empty:\n",
    "         print(\"Warning: No test samples found without structure data. Cannot perform evaluation on this subset.\")\n",
    "         # Decide how to proceed: exit, or skip evaluation part? For now, let's skip evaluation.\n",
    "         can_evaluate = False\n",
    "    else:\n",
    "         can_evaluate = True\n",
    "         X_test_seq_no_struct = prepare_sequence_data(test_data_no_struct)\n",
    "         X_test_prot_t5_no_struct = X_test_prot_t5[no_struct_mask_test]\n",
    "         y_test_no_struct = test_data_no_struct['label'].values\n",
    "\n",
    "         print(\"Test data subset (no structure) shapes:\")\n",
    "         print(f\"X_test_seq_no_struct: {X_test_seq_no_struct.shape}\")\n",
    "         print(f\"X_test_prot_t5_no_struct: {X_test_prot_t5_no_struct.shape}\")\n",
    "         print(f\"y_test_no_struct: {y_test_no_struct.shape}\")\n",
    "         print(f\"Number of test samples without structure: {len(y_test_no_struct)}\")\n",
    "         print(f\"Positive samples in this subset: {np.sum(y_test_no_struct == 1)}\")\n",
    "         print(f\"Negative samples in this subset: {np.sum(y_test_no_struct == 0)}\")\n",
    "\n",
    "\n",
    "    del test_data_aligned, X_test_prot_t5 # Free memory\n",
    "    gc.collect()\n",
    "\n",
    "    # 5. Calculate Class Weights (Based on Full Training Set)\n",
    "    total_samples_all = len(y_train_all)\n",
    "    pos_samples_all = np.sum(y_train_all == 1)\n",
    "    neg_samples_all = np.sum(y_train_all == 0)\n",
    "\n",
    "    if pos_samples_all == 0 or neg_samples_all == 0:\n",
    "        print(\"Warning: Training data contains only one class. Class weights cannot be computed effectively.\")\n",
    "        class_weights_all = None # Or {0: 1.0, 1: 1.0}\n",
    "    else:\n",
    "        class_weights_all = {\n",
    "            0: total_samples_all / (2 * neg_samples_all),\n",
    "            1: total_samples_all / (2 * pos_samples_all)\n",
    "        }\n",
    "    print(\"\\nClass weights (full training set):\", class_weights_all)\n",
    "\n",
    "    # 6. K-Fold Cross-Validation and Evaluation\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "    model_types = ['Sequence Only', 'ProtT5 Only', 'Sequence + ProtT5']\n",
    "    results = {model_name: {'cv_metrics': {m: [] for m in ['acc', 'balanced_acc', 'mcc', 'sn', 'sp']},\n",
    "                            'test_preds': []}\n",
    "               for model_name in model_types}\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_seq_all), 1):\n",
    "        print(f\"\\n===== FOLD {fold}/{n_splits} =====\")\n",
    "\n",
    "        # --- Split Data for this Fold ---\n",
    "        X_train_seq_fold, X_val_seq_fold = X_train_seq_all[train_idx], X_train_seq_all[val_idx]\n",
    "        X_train_p5_fold, X_val_p5_fold = X_train_prot_t5[train_idx], X_train_prot_t5[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train_all[train_idx], y_train_all[val_idx]\n",
    "\n",
    "        # --- Train and Evaluate Each Model ---\n",
    "        for model_name in model_types:\n",
    "            print(f\"\\n--- Training {model_name} ---\")\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "            callbacks_fold = create_callbacks()\n",
    "            model = None # Initialize model variable\n",
    "\n",
    "            if model_name == 'Sequence Only':\n",
    "                model = create_sequence_model(seq_length=seq_length, alphabet_size=alphabet_size)\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "                history = model.fit(X_train_seq_fold, y_train_fold,\n",
    "                                    validation_data=(X_val_seq_fold, y_val_fold),\n",
    "                                    epochs=epochs, batch_size=batch_size,\n",
    "                                    callbacks=callbacks_fold, class_weight=class_weights_all, verbose=0) # Verbose 0 for less output\n",
    "                # Evaluate on validation set\n",
    "                y_pred_val_proba = model.predict(X_val_seq_fold)\n",
    "                # Predict on the NO STRUCTURE TEST SET\n",
    "                if can_evaluate:\n",
    "                     test_preds_fold = model.predict(X_test_seq_no_struct)\n",
    "                     results[model_name]['test_preds'].append(test_preds_fold.flatten())\n",
    "\n",
    "\n",
    "            elif model_name == 'ProtT5 Only':\n",
    "                model = create_prot_t5_model(embedding_dim=embedding_dim)\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "                history = model.fit(X_train_p5_fold, y_train_fold,\n",
    "                                    validation_data=(X_val_p5_fold, y_val_fold),\n",
    "                                    epochs=epochs, batch_size=batch_size,\n",
    "                                    callbacks=callbacks_fold, class_weight=class_weights_all, verbose=0)\n",
    "                # Evaluate on validation set\n",
    "                y_pred_val_proba = model.predict(X_val_p5_fold)\n",
    "                # Predict on the NO STRUCTURE TEST SET\n",
    "                if can_evaluate:\n",
    "                     test_preds_fold = model.predict(X_test_prot_t5_no_struct)\n",
    "                     results[model_name]['test_preds'].append(test_preds_fold.flatten())\n",
    "\n",
    "            elif model_name == 'Sequence + ProtT5':\n",
    "                model = create_seq_prot_t5_model(seq_length=seq_length, embedding_dim=embedding_dim, alphabet_size=alphabet_size)\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "                history = model.fit([X_train_seq_fold, X_train_p5_fold], y_train_fold,\n",
    "                                    validation_data=([X_val_seq_fold, X_val_p5_fold], y_val_fold),\n",
    "                                    epochs=epochs, batch_size=batch_size,\n",
    "                                    callbacks=callbacks_fold, class_weight=class_weights_all, verbose=0)\n",
    "                # Evaluate on validation set\n",
    "                y_pred_val_proba = model.predict([X_val_seq_fold, X_val_p5_fold])\n",
    "                # Predict on the NO STRUCTURE TEST SET\n",
    "                if can_evaluate:\n",
    "                    test_preds_fold = model.predict([X_test_seq_no_struct, X_test_prot_t5_no_struct])\n",
    "                    results[model_name]['test_preds'].append(test_preds_fold.flatten())\n",
    "\n",
    "            # --- Store Validation Metrics for this fold ---\n",
    "            val_metrics = calculate_metrics(y_val_fold, y_pred_val_proba)\n",
    "            print(f\"Validation Metrics (Fold {fold}) - {model_name}: \",\n",
    "                  f\"Acc: {val_metrics['acc']:.4f}, BAcc: {val_metrics['balanced_acc']:.4f}, MCC: {val_metrics['mcc']:.4f}\")\n",
    "            for metric_name in ['acc', 'balanced_acc', 'mcc', 'sn', 'sp']:\n",
    "                results[model_name]['cv_metrics'][metric_name].append(val_metrics[metric_name])\n",
    "\n",
    "            del model, history, y_pred_val_proba, val_metrics # Clean up memory\n",
    "            if can_evaluate:\n",
    "                del test_preds_fold\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "    # 7. Final Evaluation on Test Set (No Structure Subset)\n",
    "    print(\"\\n===== FINAL EVALUATION =====\")\n",
    "\n",
    "    if not can_evaluate:\n",
    "        print(\"Skipping final evaluation because no test samples without structure were found.\")\n",
    "        return results # Return partial results\n",
    "\n",
    "    final_results = {}\n",
    "    for model_name in model_types:\n",
    "        # Average predictions across folds\n",
    "        if not results[model_name]['test_preds']:\n",
    "             print(f\"Warning: No test predictions recorded for {model_name}. Skipping final evaluation.\")\n",
    "             continue\n",
    "\n",
    "        avg_test_preds = np.mean(results[model_name]['test_preds'], axis=0)\n",
    "\n",
    "        # Calculate final metrics on the no-structure test set\n",
    "        test_metrics = calculate_metrics(y_test_no_struct, avg_test_preds)\n",
    "        final_results[model_name] = test_metrics\n",
    "\n",
    "        # Print Summary\n",
    "        print_results_summary(model_name, results[model_name]['cv_metrics'], test_metrics)\n",
    "\n",
    "    return final_results # Or return the full 'results' dict if needed\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set main parameters\n",
    "    N_SPLITS = 5\n",
    "    EPOCHS = 50 # Adjust as needed, EarlyStopping will terminate sooner\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    # Run the training and evaluation\n",
    "    final_model_results = train_evaluate_no_structure_models(\n",
    "        n_splits=N_SPLITS,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # You can access final results like:\n",
    "    # seq_only_test_mcc = final_model_results['Sequence Only']['mcc']\n",
    "    print(\"\\nScript finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-dephos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
